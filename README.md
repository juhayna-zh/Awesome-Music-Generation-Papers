# Awesome Music Generation Papers üéµ

![Awesome](https://awesome.re/badge.svg)

Curated list of groundbreaking music generation research.

### Text-to-Music Generation üìÑ‚û°üé∏


- *MusicGen* Simple and Controllable Music Generation [2024] [Meta] [[Paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/94b472a1842cd7c56dcb125fb2765fbd-Paper-Conference.pdf)] [[Code](https://github.com/facebookresearch/audiocraft)]

- MusicLM: Generating Music From Text [2023] [Google] [[Paper](https://arxiv.org/pdf/2301.11325)]


- InspireMusic: Integrating Super Resolution and Large Language Model for High-Fidelity Long-Form Music Generation [2025] [Alibaba] [[Paper](https://arxiv.org/pdf/2503.00084)] [[Code](https://github.com/FunAudioLLM/InspireMusic)] [[Demo](https://funaudiollm.github.io/inspiremusic/)]

- *FluxMusic* FLUX that Plays Music [2024] [Skywork] [[Paper](https://arxiv.org/pdf/2409.00587)] [[Code](https://github.com/feizc/FluxMusic)]

- *Magenta RT* Live Music Models [2025 [Google] [[Paper](https://arxiv.org/abs/2508.04651)]


### Lyrics-to-Song Generation üìú‚û°üé∂

- LeVo: High-Quality Song Generation with Multi-Preference Alignment [2025] [Tencent] [[Paper](https://arxiv.org/pdf/2506.07520)] [[GitHub](https://github.com/tencent-ailab/songgeneration/)] [[Demo](https://levo-demo.github.io/)]

- SongBloom: Coherent Song Generation via Interleaved Autoregressive Sketching and Diffusion Refinement [2025] [CUHK-Shenzhen] [[Paper](https://arxiv.org/pdf/2506.07634)] [[Demo](https://cypress-yang.github.io/SongBloom_demo/)] [[GitHub](https://github.com/Cypress-Yang/SongBloom)]

- ACE-Step: A Step Towards Music Generation Foundation Model
[2025] [[GitHub](https://github.com/ace-step/ACE-Step)]

- YuE: Scaling Open Foundation Models for Long-Form Music Generation [2025] [m-a-p] [[Paper](https://arxiv.org/pdf/2503.08638)] [[Code](https://github.com/multimodal-art-projection/YuE)] [[Demo](https://map-yue.github.io/) ]

- DiffRhythm: Blazingly Fast and Embarrassingly Simple End-to-End Full-Length Song Generation with Latent Diffusion [2025] [ASLP-lab] [[Paper](https://arxiv.org/pdf/2503.01183)] [[Code](https://github.com/ASLP-lab/DiffRhythm)] [[Demo](https://nzqian.github.io/DiffRhythm/)] [[HuggingFace](https://huggingface.co/spaces/ASLP-lab/DiffRhythm)]

- SongCreator: Lyrics-based Universal Song Generation [[Paper](https://proceedings.neurips.cc/paper_files/paper/2024/file/92a7a03e1c716970848a4a86cc8243ee-Paper-Conference.pdf)] [[Demo](https://thuhcsi.github.io/SongCreator/)]

- SongEditor: Adapting Zero-Shot Song Generation Language Model as a Multi-Task Editor [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/34750/36905)] [[Demo](https://cypress-yang.github.io/SongEditor_demo/)]

### Controllable Music Generation & Editing ‚öôÔ∏è

- MuseControlLite: Multifunctional Music Generation with Lightweight Conditioners [2025] [[Paper](https://arxiv.org/pdf/2506.18729)]

- Editing Music with Melody and Text: Using ControlNet for Diffusion Transformer [2024] [ICASSP] [[Paper](https://arxiv.org/pdf/2410.05151)]

- Music ControlNet: Multiple Time-Varying Controls for Music Generation [2024] [[Demo](https://musiccontrolnet.github.io/web/)]

- Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning [2024] [[Paper](https://arxiv.org/pdf/2405.18386)]

- MusiConGen: Rhythm and Chord Control for Transformer-Based Text-to-Music Generation [2024] [[Demo](https://musicongen.github.io/musicongen_demo/)]

- MusicMagus: Zero-Shot Text-to-Music Editing via Diffusion Models [2024] [[Paper](https://arxiv.org/abs/2402.06178)]

- MelodyEdit: Zero-shot Music Editing with Disentangled Inversion Control [2025] [[Paper](https://dl.acm.org/doi/10.1145/3746027.3755377)]

### Singing Voice Synthesis (SVS) üéº‚û°üé§

- DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism [2021] [[Paper](https://arxiv.org/abs/2105.02446)] [[Code](https://github.com/MoonInTheRiver/DiffSinger)]

- VISinger 2: High-Fidelity End-to-End Singing Voice Synthesis Enhanced by Digital Signal Processing Synthesizer [2022] [[Paper](https://arxiv.org/abs/2211.02903)] [[Code](https://github.com/zhangyongmao/VISinger2)]

- TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and Multi-Level Style Control [2024] [[Paper](https://arxiv.org/abs/2409.15977)] [[Code](https://github.com/AaronZ345/TCSinger)]

- TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis [2025] [[Paper](https://arxiv.org/abs/2505.14910)]

### Singing Accompaniment Generation (SAG) üé§‚û°üé∏

- FastSAG: towards fast non-autoregressive singing accompaniment generation [2024] [[Paper](https://arxiv.org/pdf/2405.07682)] [[Code](https://github.com/chenjianyi/fastsag)]

- SingSong: Generating musical accompaniments from singing [2023] [Google] [[Paper](https://arxiv.org/pdf/2301.12662)]

- Streaming Generation for Music Accompaniment [2025] [MIT] [[Paper](https://arxiv.org/pdf/2510.22105)]

### Lyrics-to-Melody Generation (L2M) üìú‚û°üéº

- CSL-L2M: Controllable Song-Level Lyric-to-Melody Generation Based on Conditional Transformer with Fine-Grained Lyric and Musical Controls [2025] [AAAI] [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/download/34523/36678) [[Code]](https://github.com/LiChaiUSTC/CSL-L2M) [[Demo]](https://lichaiustc.github.io/CSL-L2M/)

- SongComposer: A large language model for lyric and melody composition in song generation [2024] [[Paper](https://arxiv.org/pdf/2402.17645)] [[GitHub](https://github.com/pjlab-songcomposer/songcomposer)]

### Text-to-Symbolic Music Generation üìÑ‚û°üéº


- NotaGen: Advancing Musicality in Symbolic Music Generation with Large Language Model Training Paradigms  [2025] [Tsinghua] [[Paper](https://arxiv.org/pdf/2502.18008)] [[Code](https://github.com/ElectricAlexis/NotaGen)] [[Demo](https://electricalexis.github.io/notagen-demo/) ]

- Mupt: A generative symbolic music pretrained transformer [2024] [m-a-p] [[Paper](https://arxiv.org/pdf/2404.06393)] [[Demo](https://map-mupt.github.io/)]

### Music SSL & Encoder & Codec üé∂‚û°üî¢

- MuQ: Self-supervised music representation learning with mel residual vector quantization [2025] [Tencent] [[Code]](https://github.com/tencent-ailab/MuQ)

- *MusicFM* A foundation model for music informatics [2024]  [[Paper](https://arxiv.org/pdf/2311.03318)] [[Code](https://github.com/minzwon/musicfm)]

- MERT: Acoustic music understanding model with large-scale self-supervised training [2023] [m-a-p] [[Paper](https://arxiv.org/pdf/2306.00107)] [[GitHub](https://github.com/yizhilll/MERT)]

- Mucodec: Ultra low-bitrate music codec [2024] [Tencent] [[Paper](https://arxiv.org/abs/2402.06178)]


### Music Transcription üé∂‚û°üéº

- YourMT3+: Multi-Instrument Music Transcription with Enhanced Transformer Architectures and Cross-Dataset STEM Augmentation [2024] [[Paper](https://arxiv.org/pdf/2407.04822)] [[GitHub](https://github.com/mimbres/YourMT3)]

- *Perceiver TF* Multitrack music transcription with a time-frequency perceiver [2023] [ByteDance] [ICASSP] [[Paper](https://arxiv.org/pdf/2306.10785)]

- MT3: Multi-task multitrack music transcription [2021] [[Paper](https://arxiv.org/pdf/2111.03017)]

### Music Source Separation üé∂‚û°üéπüé∏ü•Å

- SCNet: Sparse compression network for music source separation [2025] [ICASSP] [[Paper](https://arxiv.org/pdf/2401.13276)]

- Music source separation with band-split rope transformer [2024] [ICASSP] [[Paper](https://arxiv.org/pdf/2309.02612)]

- Music source separation with band-split RNN [2023] [TASLP] [[Paper](https://arxiv.org/pdf/2209.15174)]

### Music Evaluation Metrics üìè

- SongEval: A Benchmark Dataset for Song Aesthetics Evaluation [2025] [ASLP-lab] [[Paper](https://arxiv.org/pdf/2505.10793)] [[GitHub](https://github.com/ASLP-lab/SongEval)] [[Dataset](https://huggingface.co/datasets/ASLP-lab/SongEval)]

- MusicEval: A Generative Music Corpus with Expert Ratings for Automatic Text-to-Music Evaluation [2025] [AISHELL] [[Paper](https://arxiv.org/pdf/2501.10811)] [[Dataset](https://www.aishelltech.com/AISHELL_7A)]

- Frechet Music Distance: A Metric For Generative Symbolic Music Evaluation [2024] [[Paper](https://arxiv.org/pdf/2412.07948)] [[GitHub](https://github.com/jryban/frechet-music-distance)]


### Dataset & Benchmark üìä

- Marble: Music audio representation benchmark for universal evaluation [2023] [m-a-p] [[Paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/7cbeec46f979618beafb4f46d8f39f36-Paper-Datasets_and_Benchmarks.pdf)] [[GitHub](https://github.com/a43992899/MARBLE-Benchmark)]

- CHORDONOMICON: A Dataset of 666,000 Songs and their Chord Progressions [2024] [[Paper](https://arxiv.org/abs/2410.22046)]

- FMA: A Dataset For Music Analysis [[GitHub]](https://github.com/mdeff/fma) [100K tracks]

### Website & Products üåê
- **Lamucal** - Real-time audio to chords, lyrics, beat, and melody with Hybrid-Net [[Web](https://lamucal.com/)] [[GitHub](https://github.com/DoMusic/Hybrid-Net)]
- **Dango.ai** - Any instruments separation and vocals extraction [[Web](https://tuanziai.com/)]
